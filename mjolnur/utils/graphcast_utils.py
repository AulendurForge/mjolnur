# -*- coding: utf-8 -*-
"""GraphCast Utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iJxpyYTzP6Jbwch4WiSNNnp1Fly6m2BW

# GraphCast Utils

### Example of how to pull in ERA5 data and prep it for use in GraphCast
"""


import xarray as xr
import numpy as np
import jax
import jax.numpy as jnp
from datetime import datetime, timedelta
import haiku as hk
from graphcast import rollout, graphcast, data_utils, checkpoint
from typing import Dict, Tuple
import dataclasses
import pandas as pd
from google.cloud import storage
import gcsfs


class GraphCastAeroTrainer:
    def __init__(self,
                 graphcast_checkpoint_path: str = None,
                 cams_zarr_path: str = None,
                 validate_graphcast: bool = True):
        """
        Initialize trainer with GraphCast model and data paths.
        """
        # Use default GraphCast Small path if not provided
        if graphcast_checkpoint_path is None:
            graphcast_checkpoint_path = (
                "GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - "
                "mesh 2to5 - precipitation input and output.npz"
            )

        # Load GraphCast Small
        self.model_config, self.task_config, self.params, self.state = self.load_graphcast(
            graphcast_checkpoint_path
        )

        # JIT-able forward will be built after we have sample shapes
        self.run_forward_jitted = None

        # CAMS data path (for later)
        self.cams_zarr_path = cams_zarr_path

        # Normalization stats
        self.norm_stats = None
        self.load_normalization_stats()

    def load_graphcast(self, checkpoint_name):
        """Load GraphCast Small checkpoint from GCS."""
        print(f"Loading checkpoint: {checkpoint_name}")

        # Anonymous client for public bucket
        gcs_client = storage.Client.create_anonymous_client()
        gcs_bucket = gcs_client.get_bucket("dm_graphcast")

        # The checkpoint file path in the bucket
        blob_path = f"params/{checkpoint_name}"
        blob = gcs_bucket.blob(blob_path)

        # Download to memory and load
        print(f"Downloading from: {blob_path}")
        with blob.open("rb") as f:
            ckpt = checkpoint.load(f, graphcast.CheckPoint)

        print("✓ Loaded model successfully!")
        print(f"  Model config: mesh_size={ckpt.model_config.mesh_size}, "
              f"resolution={ckpt.model_config.resolution}, "
              f"gnn_msg_steps={ckpt.model_config.gnn_msg_steps}")
        print(f"  Task: {len(ckpt.task_config.pressure_levels)} pressure levels, "
              f"{len(ckpt.task_config.input_variables)} inputs")

        return ckpt.model_config, ckpt.task_config, ckpt.params, {}

    def load_normalization_stats(self):
        """Load normalization statistics from GraphCast."""
        print("Loading normalization statistics...")

        gcs_client = storage.Client.create_anonymous_client()
        gcs_bucket = gcs_client.get_bucket("dm_graphcast")

        stats_files = {
            "diffs_stddev_by_level": "stats/diffs_stddev_by_level.nc",
            "mean_by_level":         "stats/mean_by_level.nc",
            "stddev_by_level":       "stats/stddev_by_level.nc",
        }

        self.norm_stats = {}
        for key, path in stats_files.items():
            blob = gcs_bucket.blob(path)
            with blob.open("rb") as f:
                self.norm_stats[key] = xr.load_dataset(f).compute()
            print(f"  ✓ Loaded {key}")

        print("✓ All normalization statistics loaded")

    def initialize_model_functions(self, sample_inputs, sample_targets, sample_forcings):
        """Initialize and (optionally) JIT compile the model functions with sample data."""
        print("Initializing model functions...")
        from graphcast import casting, normalization, autoregressive

        @hk.transform_with_state
        def run_forward(inputs, targets_template, forcings):
            predictor = graphcast.GraphCast(self.model_config, self.task_config)
            predictor = casting.Bfloat16Cast(predictor)
            predictor = normalization.InputsAndResiduals(
                predictor,
                diffs_stddev_by_level=self.norm_stats["diffs_stddev_by_level"],
                mean_by_level=self.norm_stats["mean_by_level"],
                stddev_by_level=self.norm_stats["stddev_by_level"],
            )
            predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)
            return predictor(inputs, targets_template=targets_template, forcings=forcings)

        # If you're not using a checkpoint, you'd call init() here. With a checkpoint, params are set.
        if self.params is None:
            print("Initializing model parameters from shapes...")
            self.params, self.state = run_forward.init(
                jax.random.PRNGKey(0),
                sample_inputs, sample_targets, sample_forcings
            )

        def run_forward_with_params(inputs, targets_template, forcings):
            preds, _state = run_forward.apply(
                self.params, self.state, jax.random.PRNGKey(0),
                inputs, targets_template, forcings
            )
            return preds

        # You can disable JIT if you run into PyTree/xarray issues.
        self.run_forward_jitted = jax.jit(run_forward_with_params)
        print("✓ Model functions ready (JIT compiled)")

    def prepare_era5_for_graphcast(
        self,
        start_time: str,
        num_steps: int = 4,
        lat_range: Tuple[float, float] = (-90.0, 90.0),
        lon_range: Tuple[float, float] = (0.0, 360.0),
        arco_zarr: str = "gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3",
    ) -> Dict[str, xr.Dataset]:
          """
          Fetch *real* ERA5 from ARCO-ERA5 and format it for GraphCast_small.
          Creates a 2-step (6h) context and a num_steps-step (6h) rollout.
          """
          print(f"\nPreparing ARCO-ERA5 data for GraphCast_small at start={start_time} (+{6*num_steps}h)")

          # -----------------------------
          # 0) Time planning
          # -----------------------------
          t0 = pd.Timestamp(start_time)
          hourly_start = t0 - pd.Timedelta(hours=12)                 # to compute tp6 at (t-6) and (t)
          hourly_stop  = t0 + pd.Timedelta(hours=6 * num_steps)

          # Build the desired 6‑hour stamps (context + rollout)
          sixh_times_pd = pd.date_range(start=t0 - pd.Timedelta(hours=6),
                                        end=hourly_stop, freq="6h")

          # -----------------------------
          # 1) Open ARCO-ERA5 lazily
          # -----------------------------
          ds = xr.open_dataset(
              arco_zarr,
              engine="zarr",
              storage_options={"token": "anon"},
              consolidated=True,
          ).sel(time=slice(hourly_start, hourly_stop))

          if lat_range != (-90.0, 90.0) or lon_range != (0.0, 360.0):
              ds = ds.sel(
                  latitude=slice(lat_range[1], lat_range[0]),  # ERA5 lat is descending
                  longitude=slice(lon_range[0], lon_range[1]),
              )

          # Normalize the six-hour index to the dataset's time dtype (prevents subtle dtype mismatches)
          time_dtype = getattr(ds.indexes["time"], "dtype", np.dtype("datetime64[ns]"))
          sixh_times = sixh_times_pd.to_numpy().astype(time_dtype)

          # -----------------------------
          # 2) Variables needed
          # -----------------------------
          surface_instant = [
              "2m_temperature",
              "10m_u_component_of_wind", "10m_v_component_of_wind",
              "mean_sea_level_pressure",
          ]
          level_vars = [
              "temperature", "geopotential", "u_component_of_wind",
              "v_component_of_wind", "specific_humidity", "vertical_velocity",
          ]
          pl_levels = list(self.task_config.pressure_levels)

          got_surface_inst = [v for v in surface_instant if v in ds.data_vars]
          got_level        = [v for v in level_vars        if v in ds.data_vars]

          # -----------------------------
          # 3) Instantaneous fields @ 6-hour cadence
          # -----------------------------
          inst = ds[[*got_surface_inst, *got_level]].sel(time=ds.time.dt.hour.isin([0, 6, 12, 18]))
          if got_level:
              inst = inst.sel(level=pl_levels)

          # Force *identical* time index (length & dtype) before any merge
          inst = inst.reindex(time=sixh_times).sortby("time")

          # -----------------------------
          # 4) 6-hour precipitation accumulations
          # -----------------------------
          tp_name = None
          for cand in ["total_precipitation", "tp"]:
              if cand in ds.data_vars:
                  tp_name = cand
                  break
          if tp_name is None:
              raise ValueError("Hourly total precipitation not found in ARCO dataset "
                              "(tried 'total_precipitation' and 'tp').")

          tp6 = ds[tp_name].rolling(time=6, min_periods=6).sum()
          tp6 = tp6.sel(time=tp6.time.dt.hour.isin([0, 6, 12, 18]))
          tp6 = tp6.reindex(time=sixh_times).sortby("time")
          tp6 = tp6.to_dataset(name="total_precipitation_6hr")

          # -----------------------------
          # 5) STATIC fields (no 'time' dim)
          # -----------------------------
          static_candidates = []
          if "geopotential_at_surface" in ds.data_vars:
              static_candidates.append("geopotential_at_surface")
          elif "geopotential" in ds.data_vars and "level" not in ds["geopotential"].dims:
              static_candidates.append("geopotential")  # will rename to _at_surface
          if "land_sea_mask" in ds.data_vars:
              static_candidates.append("land_sea_mask")

          stat_025 = xr.Dataset()
          if static_candidates:
              stat_025 = ds[static_candidates]
              for name in list(stat_025.data_vars):
                  if "time" in stat_025[name].dims:
                      stat_025[name] = stat_025[name].isel(time=0, drop=True)

          # -----------------------------
          # 6) Coarsen 0.25° → 1° and rename coords
          # -----------------------------
          def coarsen_to_1deg(dset: xr.Dataset) -> xr.Dataset:
              # Use sizes to avoid future warnings on .dims
              if not all(k in getattr(dset, "sizes", {}) for k in ("latitude", "longitude")):
                  return dset
              factor = int(1.0 / 0.25)
              d = dset.coarsen(latitude=factor, longitude=factor, boundary="trim").mean()
              return d.rename({"latitude": "lat", "longitude": "lon"})

          inst_1deg = coarsen_to_1deg(inst)
          tp6_1deg  = coarsen_to_1deg(tp6)
          stat_1deg = coarsen_to_1deg(stat_025) if len(stat_025.data_vars) else stat_025

          # Normalize static variable names
          if "geopotential" in stat_1deg.data_vars and "geopotential_at_surface" not in stat_1deg:
              stat_1deg = stat_1deg.rename({"geopotential": "geopotential_at_surface"})

          # Ensure statics have NO time dimension
          for name in ["geopotential_at_surface", "land_sea_mask"]:
              if name in stat_1deg and "time" in stat_1deg[name].dims:
                  stat_1deg[name] = stat_1deg[name].isel(time=0, drop=True)

          # Drop any time-broadcast statics from inst (safety)
          inst_1deg = inst_1deg.drop_vars(
              [v for v in ["geopotential_at_surface", "land_sea_mask"] if v in inst_1deg],
              errors="ignore",
          )

          # -----------------------------
          # 7) HARD unify 'time' before merge (prevents AlignmentError)
          # -----------------------------
          # Assign the *same* coordinate object to both time-dependent datasets
          inst_1deg = inst_1deg.assign_coords(time=("time", sixh_times))
          tp6_1deg  = tp6_1deg.assign_coords(time=("time", sixh_times))

          # Optional: sanity assertions to fail fast if something drifted
          assert "time" in inst_1deg.sizes and inst_1deg.sizes["time"] == len(sixh_times)
          assert "time" in tp6_1deg.sizes  and tp6_1deg.sizes["time"]  == len(sixh_times)

          print(f"inst_1deg times: {inst_1deg.time.values[:3]}... shape={len(inst_1deg.time)}")
          print(f"tp6_1deg times: {tp6_1deg.time.values[:3]}... shape={len(tp6_1deg.time)}")
          print(f"Expected times: {sixh_times[:3]}... shape={len(sixh_times)}")

          # Now the times truly match 1:1; require exact match at merge
          data_6h = xr.merge([inst_1deg, tp6_1deg, stat_1deg], compat="override", join="inner")

          # -----------------------------
          # 8) Dim order & coords
          # -----------------------------
          for v in list(data_6h.data_vars):
              dims = data_6h[v].dims
              if ("level" in dims) and (dims != ("time", "level", "lat", "lon")):
                  data_6h[v] = data_6h[v].transpose("time", "level", "lat", "lon")
              elif ("level" not in dims) and ("time" in dims) and (dims != ("time", "lat", "lon")):
                  data_6h[v] = data_6h[v].transpose("time", "lat", "lon")
              elif ("level" not in dims) and ("time" not in dims) and (dims != ("lat", "lon")):
                  data_6h[v] = data_6h[v].transpose("lat", "lon")

          # Provide both 'time' and 'datetime' (GraphCast utils sometimes expect both)
          data_6h = data_6h.assign_coords(datetime=("time", data_6h.time.values))

          # Keep only what the task needs (+ our computed precip_6hr)
          required = set(self.task_config.input_variables) | set(self.task_config.target_variables)
          keep = [v for v in data_6h.data_vars if (v in required) or (v == "total_precipitation_6hr")]
          data_6h = data_6h[keep]

          # -----------------------------
          # 9) Build (inputs, targets, forcings) exactly like the demo
          # -----------------------------
          from graphcast import data_utils as du

          tc = dataclasses.asdict(self.task_config)
          cfg = {
              "input_variables":   tuple(tc["input_variables"]),
              "target_variables":  tuple(tc["target_variables"]),
              "forcing_variables": tuple(tc["forcing_variables"]),
              "pressure_levels":   tuple(tc["pressure_levels"]),
          }

          print(f"data_6h time dimension: {data_6h.sizes['time']} timesteps")
          print(f"data_6h times: {data_6h.time.values}")

          # 6h context (2 steps) + K-step rollout
          inputs, targets, forcings = du.extract_inputs_targets_forcings(
              data_6h,
              input_duration="12h",
              target_lead_times=slice("6h", f"{6*num_steps}h"),
              **cfg,
          )

          # Batch-first
          inputs   = inputs.expand_dims(dim="batch", axis=0)
          targets  = targets.expand_dims(dim="batch", axis=0)
          if forcings is not None:
              forcings = forcings.expand_dims(dim="batch", axis=0)

          # Sanity: inputs time = context(2) + rollout(num_steps)
          expected_inputs_time = 2 + num_steps
          # Sanity: inputs should have 2 timesteps (context), targets should have num_steps
          assert inputs.sizes["time"] == 2, (
              f"Expected 2 input times (context), got {inputs.sizes['time']}"
          )
          assert targets.sizes["time"] == num_steps, (
              f"Expected {num_steps} target steps, got {targets.sizes['time']}"
          )
          for name in ["geopotential_at_surface", "land_sea_mask"]:
              if name in inputs and "time" in inputs[name].dims:
                  raise RuntimeError(f"Static input {name} has a time dim; expected none.")

          print("  Inputs:",   {k: tuple(v.sizes.values()) for k, v in inputs.data_vars.items()})
          print("  Targets:",  {k: tuple(v.sizes.values()) for k, v in targets.data_vars.items()})
          print("  Forcings:", None if forcings is None else {k: tuple(v.sizes.values()) for k, v in forcings.data_vars.items()})

          return {"inputs": inputs, "targets": targets, "forcings": forcings, "raw_data": data_6h}


    def run_graphcast_prediction(self, era5_data: Dict) -> xr.Dataset:
        """Run GraphCast forward to generate predictions."""
        print("\nRunning GraphCast prediction...")

        # Initialize model if needed
        if self.run_forward_jitted is None:
            self.initialize_model_functions(
                era5_data["inputs"],
                era5_data["targets"],
                era5_data["forcings"],
            )

        # Prediction (targets_template provides structure/lead times)
        predictions = self.run_forward_jitted(
            inputs=era5_data["inputs"],
            targets_template=era5_data["targets"] * np.nan,
            forcings=era5_data["forcings"],
        )

        print(f"✓ Generated predictions with dims: {dict(predictions.sizes)}")
        return predictions

    def validate_graphcast_predictions(self,
                                      predictions: xr.Dataset,
                                      targets: xr.Dataset) -> Dict:
        """Validate GraphCast predictions against targets."""
        print("\nValidating predictions...")
        metrics = {}

        check_vars = ["2m_temperature", "10m_u_component_of_wind", "mean_sea_level_pressure"]
        for var in check_vars:
            if var in predictions and var in targets:
                pred = predictions[var].values
                targ = targets[var].values
                rmse = float(np.sqrt(np.mean((pred - targ) ** 2)))
                mae  = float(np.mean(np.abs(pred - targ)))
                metrics[var] = {
                    "rmse": rmse,
                    "mae": mae,
                    "pred_mean": float(pred.mean()),
                    "pred_std": float(pred.std()),
                    "target_mean": float(targ.mean()),
                    "target_std": float(targ.std()),
                }
                print(f"  {var}: RMSE={rmse:.3f}, MAE={mae:.3f}")
        return metrics

    def test_model_loading(self):
        """End-to-end test that loads ARCO-ERA5 and runs a GraphCast forward pass."""
        print("\n" + "="*50)
        print("TESTING GRAPHCAST MODEL WITH ARCO-ERA5 INPUTS")
        print("="*50)

        test_data = self.prepare_era5_for_graphcast("2014-01-01T00:00:00", num_steps=4)
        predictions = self.run_graphcast_prediction(test_data)
        _ = self.validate_graphcast_predictions(predictions, test_data["targets"])

        print("\n✓ GraphCast model ran successfully with ARCO-ERA5 inputs.")
        return {"ok": True}


# Test the loading and basic functionality
if __name__ == "__main__":
    # Use a date comfortably within ERA5 coverage & GraphCast_small norms
    START = "2014-01-01T00:00:00"
    trainer = GraphCastAeroTrainer(
        graphcast_checkpoint_path=(
            "GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - "
            "mesh 2to5 - precipitation input and output.npz"
        ),
    )
    # Build a real batch from ARCO-ERA5 and run a forward pass
    era5_batch = trainer.prepare_era5_for_graphcast(START, num_steps=4)  # 24h horizon
    preds = trainer.run_graphcast_prediction(era5_batch)

    # Quick checks: no NaNs, correct time stamps, shapes match targets
    print("\nQuick sanity checks:")
    print("  pred time coords:", preds.time.values)
    for v in ["2m_temperature", "mean_sea_level_pressure", "total_precipitation_6hr"]:
        if v in preds:
            assert np.isfinite(preds[v]).all(), f"NaNs in predictions for {v}"
            print(f"  {v}: pred shape {tuple(preds[v].shape)}")

    print("\n✓ End-to-end GraphCast_small inference succeeded with ARCO-ERA5 inputs.")

"""### Example of how to pull in CAMS Data"""


#!/usr/bin/env python3
"""
CAMS Zarr Dataset Interactive Viewer
Visualize PM2.5 and PM10 data from your fixed Zarr store
"""

import os
import numpy as np
import pandas as pd
import xarray as xr
import gcsfs
import ipywidgets as W
from IPython.display import display, clear_output
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import cm
from matplotlib.colors import LinearSegmentedColormap
import warnings
warnings.filterwarnings('ignore')

# High quality plots
mpl.rcParams['figure.figsize'] = (12, 8)
mpl.rcParams['figure.dpi'] = 100
mpl.rcParams['font.size'] = 10
mpl.rcParams['axes.labelsize'] = 11
mpl.rcParams['axes.titlesize'] = 12
mpl.rcParams['xtick.labelsize'] = 10
mpl.rcParams['ytick.labelsize'] = 10

# Configuration
BUCKET = os.getenv("MJOLNUR_BUCKET", "mjolnur-cams-data")
ZARR_URI = f"gs://{BUCKET}/cams/zarr/pm_1deg_conservative.zarr"

# ============ Initialize GCS and Load Dataset ============
print("Initializing CAMS Zarr Viewer...")
print(f"Connecting to: {ZARR_URI}")

from google.colab import auth
auth.authenticate_user()

fs = gcsfs.GCSFileSystem(token="google_default")
store = fs.get_mapper(ZARR_URI)

# Load the dataset
print("Loading dataset...")
try:
    ds = xr.open_zarr(store, consolidated=True)
    print(f"✓ Dataset loaded successfully")
    print(f"  Dimensions: {dict(ds.sizes)}")
    print(f"  Variables: {list(ds.data_vars)}")

    # Fix time coordinate
    print("Fixing time coordinate...")

    # The time values are seconds since 1970-01-01
    # Convert to datetime and ensure it's sorted
    time_values = ds.time.values

    # Convert seconds to datetime
    time_datetime = pd.to_datetime(time_values, unit='s')

    # Check if time needs sorting
    if not time_datetime.is_monotonic_increasing:
        print("  Sorting time coordinate...")
        sort_indices = np.argsort(time_values)
        ds = ds.isel(time=sort_indices)
        time_datetime = pd.to_datetime(ds.time.values, unit='s')

    # Replace time coordinate with properly decoded datetime
    ds['time'] = time_datetime

    # Verify the time range
    time_range = pd.to_datetime(ds.time.values)
    print(f"  Time range: {time_range[0]} to {time_range[-1]}")
    print(f"  Time steps: {len(time_range)} ({len(time_range)/4:.0f} days of 6-hourly data)")

    # Check time spacing
    time_diff = pd.Series(time_range).diff().dropna().unique()
    if len(time_diff) == 1:
        print(f"  Time spacing: {time_diff[0]}")
    else:
        print(f"  Variable time spacing detected: {len(time_diff)} unique intervals")

except Exception as e:
    print(f"✗ Error loading dataset: {e}")
    raise